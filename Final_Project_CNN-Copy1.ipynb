{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedder:\n",
    "    def __init__(self, d=768, N=3, B=int(1e9+7), random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the TokenEmbedder with given parameters.\n",
    "        \n",
    "        :param d: Size of the embedding vector.\n",
    "        :param N: Number of i-grams to consider.\n",
    "        :param B: The modulus for the hash function and projection matrix normalization.\n",
    "        :param random_state: Seed for random number generation to ensure reproducibility.\n",
    "      `  \"\"\"\n",
    "        self.d = d\n",
    "        self.N = N\n",
    "        self.B = B\n",
    "        self.random_state = random_state\n",
    "        self.hash_seeds = self.initialize_hash_seeds()\n",
    "        self.embedding_dict = {}\n",
    "\n",
    "    def initialize_hash_seeds(self):\n",
    "        \"\"\"Initialize hash seeds with a fixed random state.\"\"\"\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        return rng.randint(low=1, high=np.iinfo(np.int32).max, size=self.d, dtype=np.int32)\n",
    "\n",
    "    def rolling_hash(self, text, i, base=256):\n",
    "        \"\"\"Compute rolling hash for all i-grams in a text.\"\"\"\n",
    "        h = 0\n",
    "        for x in range(i):\n",
    "            h = (h * base + ord(text[x])) % self.B\n",
    "        yield h\n",
    "        for x in range(len(text) - i):\n",
    "            h = (h * base - ord(text[x]) * pow(base, i, self.B) + ord(text[x + i])) % self.B\n",
    "            yield h\n",
    "\n",
    "    def compute_projection_matrix(self, s_i, h_i):\n",
    "        \"\"\"Compute and transform the projection matrix for i-grams.\"\"\"\n",
    "        P_i = np.outer(s_i, h_i) % self.B\n",
    "        P_i = P_i.astype(np.float64)  # Convert P_i to float64 before the division\n",
    "        P_i -= (P_i > self.B // 2) * self.B\n",
    "        P_i /= (self.B // 2)\n",
    "        return P_i\n",
    "\n",
    "    def compute_igram_embedding(self, P_i):\n",
    "        \"\"\"Compute the embedding for an i-gram by averaging the projection matrix.\"\"\"\n",
    "        return np.mean(P_i, axis=0)\n",
    "\n",
    "    def generate_token_embedding(self, embeddings):\n",
    "        \"\"\"Concatenate all i-gram embeddings to form the token embedding.\"\"\"\n",
    "        token_embedding = np.concatenate(embeddings, axis=0)\n",
    "        return token_embedding\n",
    "\n",
    "    def compute_embedding(self, token):\n",
    "        \"\"\"\n",
    "        Compute the embedding for a given token by orchestrating the entire process.\n",
    "        \n",
    "        :param token: The token for which to compute the embedding.\n",
    "        :return: The computed embedding vector for the token.\n",
    "        \"\"\"\n",
    "        if token not in self.embedding_dict:\n",
    "            embeddings = []\n",
    "            l = len(token)\n",
    "            partitions = np.array_split(self.hash_seeds, self.N)  # Partitioning hash seeds for N i-grams\n",
    "\n",
    "            for i, h_i in enumerate(partitions, start=1):\n",
    "                if len(token) < i:  # Check if token is shorter than i\n",
    "                    # Handle short tokens; options might include skipping or using a different approach\n",
    "                    continue  # Skipping in this example\n",
    "\n",
    "                s_i = np.array(list(self.rolling_hash(token, i)))\n",
    "                P_i = self.compute_projection_matrix(s_i, h_i)\n",
    "                e_i = self.compute_igram_embedding(P_i)\n",
    "                embeddings.append(e_i)\n",
    "\n",
    "            if not embeddings:  # If no embeddings were generated (e.g., all tokens were too short)\n",
    "                return np.zeros(self.d)  # Return a zero vector of size d\n",
    "\n",
    "            token_embedding = self.generate_token_embedding(embeddings)\n",
    "            self.embedding_dict[token] = token_embedding\n",
    "            return token_embedding\n",
    "        else:\n",
    "            return self.embedding_dict[token]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/krt4vhn17ls6fxrd79rqftk00000gn/T/ipykernel_63640/821680044.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('amazon_reviews_us_Office_Products_v1_00.tsv',on_bad_lines='skip', sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('amazon_reviews_us_Office_Products_v1_00.tsv',on_bad_lines='skip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>52782374</td>\n",
       "      <td>R1PR37BR7G3M6A</td>\n",
       "      <td>B00D7H8XB6</td>\n",
       "      <td>868449945</td>\n",
       "      <td>AmazonBasics 12-Sheet High-Security Micro-Cut ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>24045652</td>\n",
       "      <td>R3BDDDZMZBZDPU</td>\n",
       "      <td>B001XCWP34</td>\n",
       "      <td>33521401</td>\n",
       "      <td>Derwent Colored Pencils, Inktense Ink Pencils,...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>US</td>\n",
       "      <td>53005790</td>\n",
       "      <td>RLI7EI10S7SN0</td>\n",
       "      <td>B00000DM9M</td>\n",
       "      <td>223408988</td>\n",
       "      <td>PalmOne III Leather Belt Clip Case</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Great value! A must if you hate to carry thing...</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "      <td>1998-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>US</td>\n",
       "      <td>52188548</td>\n",
       "      <td>R1F3SRK9MHE6A3</td>\n",
       "      <td>B00000DM9M</td>\n",
       "      <td>223408988</td>\n",
       "      <td>PalmOne III Leather Belt Clip Case</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Attaches the Palm Pilot like an appendage</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "      <td>1998-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>US</td>\n",
       "      <td>52090046</td>\n",
       "      <td>R23V0C4NRJL8EM</td>\n",
       "      <td>0807865001</td>\n",
       "      <td>307284585</td>\n",
       "      <td>Gods and Heroes of Ancient Greece</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Excellent information, pictures and stories, I...</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "      <td>1998-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>US</td>\n",
       "      <td>52503173</td>\n",
       "      <td>R13ZAE1ATEUC1T</td>\n",
       "      <td>1572313188</td>\n",
       "      <td>870359649</td>\n",
       "      <td>Microsoft EXCEL 97/ Visual Basic Step-by-Step ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>class text</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "      <td>1998-08-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>US</td>\n",
       "      <td>52585611</td>\n",
       "      <td>RE8J5O2GY04NN</td>\n",
       "      <td>1572313188</td>\n",
       "      <td>870359649</td>\n",
       "      <td>Microsoft EXCEL 97/ Visual Basic Step-by-Step ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Microsoft's Finest</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "      <td>1998-07-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640254 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0                US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1                US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2                US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "3                US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
       "4                US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
       "...             ...          ...             ...         ...             ...   \n",
       "2640249          US     53005790   RLI7EI10S7SN0  B00000DM9M       223408988   \n",
       "2640250          US     52188548  R1F3SRK9MHE6A3  B00000DM9M       223408988   \n",
       "2640251          US     52090046  R23V0C4NRJL8EM  0807865001       307284585   \n",
       "2640252          US     52503173  R13ZAE1ATEUC1T  1572313188       870359649   \n",
       "2640253          US     52585611   RE8J5O2GY04NN  1572313188       870359649   \n",
       "\n",
       "                                             product_title product_category  \\\n",
       "0           Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1                Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2        Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "3        AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
       "4        Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
       "...                                                    ...              ...   \n",
       "2640249                 PalmOne III Leather Belt Clip Case  Office Products   \n",
       "2640250                 PalmOne III Leather Belt Clip Case  Office Products   \n",
       "2640251                  Gods and Heroes of Ancient Greece  Office Products   \n",
       "2640252  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
       "2640253  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
       "\n",
       "        star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0                 5            0.0          0.0    N                 Y   \n",
       "1                 5            0.0          1.0    N                 Y   \n",
       "2                 5            0.0          0.0    N                 Y   \n",
       "3                 1            2.0          3.0    N                 Y   \n",
       "4                 4            0.0          0.0    N                 Y   \n",
       "...             ...            ...          ...  ...               ...   \n",
       "2640249           4           26.0         26.0    N                 N   \n",
       "2640250           4           18.0         18.0    N                 N   \n",
       "2640251           4            9.0         16.0    N                 N   \n",
       "2640252           5            0.0          0.0    N                 N   \n",
       "2640253           5            0.0          0.0    N                 N   \n",
       "\n",
       "                                           review_headline  \\\n",
       "0                                               Five Stars   \n",
       "1        Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                            but I am sure I will like it.   \n",
       "3        and the shredder was dirty and the bin was par...   \n",
       "4                                               Four Stars   \n",
       "...                                                    ...   \n",
       "2640249  Great value! A must if you hate to carry thing...   \n",
       "2640250          Attaches the Palm Pilot like an appendage   \n",
       "2640251  Excellent information, pictures and stories, I...   \n",
       "2640252                                         class text   \n",
       "2640253                                 Microsoft's Finest   \n",
       "\n",
       "                                               review_body review_date  \n",
       "0                                           Great product.  2015-08-31  \n",
       "1        What's to say about this commodity item except...  2015-08-31  \n",
       "2          Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
       "3        Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
       "4                          Gorgeous colors and easy to use  2015-08-31  \n",
       "...                                                    ...         ...  \n",
       "2640249  I can't live anymore whithout my Palm III. But...  1998-12-07  \n",
       "2640250  Although the Palm Pilot is thin and compact it...  1998-11-30  \n",
       "2640251  This book had a lot of great content without b...  1998-10-15  \n",
       "2640252  I am teaching a course in Excel and am using t...  1998-08-22  \n",
       "2640253  A very comprehensive layout of exactly how Vis...  1998-07-15  \n",
       "\n",
       "[2640254 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews with Rating > 3: 2001122\n",
      "Number of Reviews with Rating <= 2: 445349\n",
      "Number of Reviews with Rating = 3: 193686\n"
     ]
    }
   ],
   "source": [
    "# keep only reviews and ratings\n",
    "df = df[['star_rating', 'review_body']]\n",
    "\n",
    "# Check for null values in the df\n",
    "df.isnull().any(axis=1).sum()\n",
    "df = df.dropna()\n",
    "\n",
    "# it seems that some values of star_rating are string while some are numeric. the below code will give an error and hence i was able to deduce this\n",
    "# df['sentiment'] = df['star_rating'].map(lambda x: 1 if x > 3 else 0 if x <= 2 else None)\n",
    "# df.shape\n",
    "\n",
    "# Convert 'star_rating' to numeric\n",
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "df['star_rating'] = df['star_rating'].astype(int)\n",
    "\n",
    "# Get counts of reviews for each sentiment class\n",
    "reviews_greater_than_3 = df[df['star_rating'] > 3].shape[0]\n",
    "reviews_less_than_equal_2 = df[df['star_rating'] <= 2].shape[0]\n",
    "reviews_equal_3 = df[df['star_rating'] == 3].shape[0]\n",
    "\n",
    "print(\"Number of Reviews with Rating > 3:\", reviews_greater_than_3)\n",
    "print(\"Number of Reviews with Rating <= 2:\", reviews_less_than_equal_2)\n",
    "print(\"Number of Reviews with Rating = 3:\", reviews_equal_3)\n",
    "\n",
    "# create sentiment column\n",
    "df['sentiment'] = df['star_rating'].map(lambda x: 0 if x > 3 else 1 if x <= 2 else 2 if x == 3 else None)\n",
    "\n",
    "\n",
    "# convert sentiment to int type\n",
    "df['sentiment'] = df['sentiment'].astype(int)\n",
    "\n",
    "rating_one = df[df['star_rating'] == 1].sample(n=50000, random_state=42)\n",
    "rating_two = df[df['star_rating'] == 2].sample(n=50000, random_state=42)\n",
    "rating_three = df[df['star_rating'] == 3].sample(n=50000, random_state=42)\n",
    "rating_four = df[df['star_rating'] == 4].sample(n=50000, random_state=42)\n",
    "rating_five = df[df['star_rating'] == 5].sample(n=50000, random_state=42)\n",
    "\n",
    "downsized_df = pd.concat([rating_one, rating_two, rating_three, rating_four, rating_five])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\"ain't\": 'am not / is not / are not / has not / have not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would / he had', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he will have', \"he's\": 'he is / he has', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would / I had', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"isn't\": 'is not', \"it'd\": 'it would / it had', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is / it has', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would / she had', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is / she has', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so is', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is / that has', \"there'd\": 'there had', \"there'd've\": 'there would have', \"there's\": 'there is / there has', \"they'd\": 'they would / they had', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would / we had', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is / what has', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is / where has', \"where've\": 'where have', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is / who has', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'alls\": 'you alls', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would / you had', \"you'd've\": 'you would have', \"you'll\": 'you you will', \"you'll've\": 'you you will have', \"you're\": 'you are', \"you've\": 'you have', \"who'd\": 'who would / who had', \"who're\": 'who are'}\n",
    "\n",
    "def expand_contractions(text):\n",
    "     for contraction, expansion_options in contractions.items():\n",
    "        # Select the first option when there are multiple choices\n",
    "        first_option = expansion_options.split('/')[0].strip()\n",
    "        text = text.replace(contraction, first_option)\n",
    "     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/krt4vhn17ls6fxrd79rqftk00000gn/T/ipykernel_63640/803437059.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n",
      "/var/folders/9p/krt4vhn17ls6fxrd79rqftk00000gn/T/ipykernel_63640/803437059.py:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n"
     ]
    }
   ],
   "source": [
    "downsized_df['review_body'] = downsized_df['review_body'].str.lower()\n",
    "downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n",
    "downsized_df['review_body'] = downsized_df['review_body'].str.replace('http[s]?://\\S+', '', regex=True)\n",
    "downsized_df['review_body'] = downsized_df['review_body'].str.replace(r'[^a-zA-Z ]', '', regex=True)\n",
    "downsized_df['review_body'] = downsized_df['review_body'].str.replace(' +', ' ', regex=True)\n",
    "downsized_df['review_body'] = downsized_df['review_body'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2520757</th>\n",
       "      <td>1</td>\n",
       "      <td>i bought this thinking it would be a good alte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800253</th>\n",
       "      <td>1</td>\n",
       "      <td>i have used transfers many times in the past w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366081</th>\n",
       "      <td>1</td>\n",
       "      <td>sorry to report that this ink cartridge spille...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159817</th>\n",
       "      <td>1</td>\n",
       "      <td>doesnt work dont buy i tried two times at inst...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118644</th>\n",
       "      <td>1</td>\n",
       "      <td>i replaced the original canon toner cartridge ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840862</th>\n",
       "      <td>5</td>\n",
       "      <td>a unique gift that helps save those amazing cr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281476</th>\n",
       "      <td>5</td>\n",
       "      <td>exactly as described i got two of these one in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804044</th>\n",
       "      <td>5</td>\n",
       "      <td>these are much better than some other brands i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221856</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent supplier just as promised and timely...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564798</th>\n",
       "      <td>5</td>\n",
       "      <td>good product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  \\\n",
       "2520757            1  i bought this thinking it would be a good alte...   \n",
       "800253             1  i have used transfers many times in the past w...   \n",
       "2366081            1  sorry to report that this ink cartridge spille...   \n",
       "1159817            1  doesnt work dont buy i tried two times at inst...   \n",
       "2118644            1  i replaced the original canon toner cartridge ...   \n",
       "...              ...                                                ...   \n",
       "840862             5  a unique gift that helps save those amazing cr...   \n",
       "1281476            5  exactly as described i got two of these one in...   \n",
       "804044             5  these are much better than some other brands i...   \n",
       "221856             5  excellent supplier just as promised and timely...   \n",
       "564798             5                                       good product   \n",
       "\n",
       "         sentiment  \n",
       "2520757          1  \n",
       "800253           1  \n",
       "2366081          1  \n",
       "1159817          1  \n",
       "2118644          1  \n",
       "...            ...  \n",
       "840862           0  \n",
       "1281476          0  \n",
       "804044           0  \n",
       "221856           0  \n",
       "564798           0  \n",
       "\n",
       "[250000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsized_df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = TokenEmbedder(d=300, N=3, B=int(1e9+7), random_state=42) # Adjust the dimensions and parameters as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_X_avg_custom_with_embedder(df, embedder):\n",
    "#     X_avg = []\n",
    "\n",
    "#     for review in df['review_body']:\n",
    "#         # Tokenize the review text\n",
    "#         curr_review = review.replace(',', '').replace('.', '').split()\n",
    "#         embeddings = []\n",
    "\n",
    "#         for token in curr_review:\n",
    "#             # Generate an embedding for each token and ensure it's properly shaped\n",
    "#             token_embedding = embedder.compute_embedding(token)\n",
    "#             if token_embedding.shape[0] == embedder.d:  # Check if embedding has expected length\n",
    "#                 embeddings.append(token_embedding)\n",
    "\n",
    "#         # Ensure embeddings is a 2D array with consistent inner dimension\n",
    "#         if len(embeddings) > 0:\n",
    "#             embeddings = np.vstack(embeddings)  # Stack embeddings vertically to create a 2D array\n",
    "#             # Average the embeddings for the review\n",
    "#             review_embedding = np.mean(embeddings, axis=0)\n",
    "#         else:\n",
    "#             # If no valid embeddings were generated, use a zero vector\n",
    "#             review_embedding = np.zeros(embedder.d)\n",
    "\n",
    "#         X_avg.append(review_embedding)\n",
    "\n",
    "#     return np.array(X_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_avg_custom = create_X_avg_custom_with_embedder(downsized_df, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_avg_custom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_custom, X_test_custom, Y_train_custom, Y_test_custom = train_test_split(X_avg_custom, downsized_df['sentiment'], test_size=0.2, random_state=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw_binary, X_test_raw_binary, Y_train_raw_binary, Y_test_raw_binary = train_test_split(downsized_df['review_body'], downsized_df['sentiment'], test_size=0.2, random_state=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainReview(Dataset):\n",
    "#     def __init__(self, reviews, sentiment, token_embedder):\n",
    "#         \"\"\"\n",
    "#         Initializes the dataset with reviews, sentiment labels, and a token embedder.\n",
    "        \n",
    "#         :param reviews: A pandas Series or DataFrame column containing review texts.\n",
    "#         :param sentiment: A pandas Series or DataFrame column containing sentiment labels.\n",
    "#         :param token_embedder: An instance of the TokenEmbedder class.\n",
    "#         \"\"\"\n",
    "#         self.reviews = reviews\n",
    "#         self.sentiment = sentiment\n",
    "#         self.token_embedder = token_embedder\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.reviews)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         curr_review = self.reviews.iloc[index]\n",
    "#         curr_review = curr_review.replace(',', '').replace('.', '').split()\n",
    "#         curr_vect = []\n",
    "\n",
    "#         for word in curr_review:\n",
    "#             # Use the TokenEmbedder's compute_embedding method to get the embedding\n",
    "#             word_embedding = self.token_embedder.compute_embedding(word)\n",
    "#             if word_embedding.shape[0] == self.token_embedder.d:\n",
    "#                 curr_vect.append(word_embedding)\n",
    "#         if len(curr_vect) == 0:\n",
    "#             curr_vect = np.zeros(self.token_embedder.d, dtype=float)  # Use the embedder's dimension\n",
    "#         else:\n",
    "#             curr_vect = np.mean(np.array(curr_vect), axis=0)\n",
    "\n",
    "#         # Convert to pytorch tensor\n",
    "#         curr_vect = torch.from_numpy(curr_vect).float()\n",
    "#         sentiment = self.sentiment.iloc[index]\n",
    "\n",
    "#         return curr_vect, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestReview(Dataset):\n",
    "#     def __init__(self, reviews, sentiment, token_embedder):\n",
    "#         \"\"\"\n",
    "#         Initializes the dataset with reviews, sentiment labels, and a token embedder.\n",
    "        \n",
    "#         :param reviews: A pandas Series or DataFrame column containing review texts.\n",
    "#         :param sentiment: A pandas Series or DataFrame column containing sentiment labels.\n",
    "#         :param token_embedder: An instance of the TokenEmbedder class for generating embeddings.\n",
    "#         \"\"\"\n",
    "#         self.reviews = reviews\n",
    "#         self.sentiment = sentiment\n",
    "#         self.token_embedder = token_embedder\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.reviews)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         curr_review = self.reviews.iloc[index]\n",
    "#         curr_review = curr_review.replace(',', '').replace('.', '').split()\n",
    "#         curr_vect = []\n",
    "\n",
    "#         for word in curr_review:\n",
    "#             # Use the TokenEmbedder's compute_embedding method to get the embedding for each word\n",
    "#             word_embedding = self.token_embedder.compute_embedding(word)\n",
    "#             if word_embedding.shape[0] == self.token_embedder.d:\n",
    "                \n",
    "#                 curr_vect.append(word_embedding)\n",
    "\n",
    "#         if len(curr_vect) == 0:\n",
    "#             curr_vect = np.zeros(self.token_embedder.d, dtype=float)  # Use the embedder's dimension\n",
    "#         else:\n",
    "#             curr_vect = np.mean(np.array(curr_vect), axis=0)\n",
    "\n",
    "#         # Convert to PyTorch tensor\n",
    "#         curr_vect = torch.from_numpy(curr_vect).float()\n",
    "#         sentiment = self.sentiment.iloc[index]\n",
    "\n",
    "#         return curr_vect, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_avg_custom_binary = TrainReview(X_train_raw_binary, Y_train_raw_binary, embedder)\n",
    "# test_data_avg_custom_binary = TestReview(X_test_raw_binary, Y_test_raw_binary, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how many samples per batch to load\n",
    "# batch_size = 100\n",
    "# # percentage of training set to use as validation\n",
    "# valid_size = 0.2\n",
    "\n",
    "# # obtain training indices that will be used for validation\n",
    "# num_train = len(train_data_avg_custom_binary)\n",
    "# indices = list(range(num_train))\n",
    "# np.random.shuffle(indices)\n",
    "# split = int(np.floor(valid_size * num_train))\n",
    "# train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# # define samplers for obtaining training and validation batches\n",
    "# train_sampler = SubsetRandomSampler(train_idx)\n",
    "# valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# # prepare data loaders\n",
    "# train_loader = torch.utils.data.DataLoader(train_data_avg_custom_binary, batch_size=batch_size, sampler=train_sampler)\n",
    "# valid_loader = torch.utils.data.DataLoader(train_data_avg_custom_binary, batch_size=batch_size, sampler=valid_sampler)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data_avg_custom_binary, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw_binary, X_test_raw_binary, Y_train_raw_binary, Y_test_raw_binary = train_test_split(downsized_df['review_body'], downsized_df['sentiment'], test_size=0.2, random_state=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainReviewCNN(Dataset):\n",
    "    def __init__(self, reviews, sentiment, token_embedder, max_length=50, vector_size=300):\n",
    "        self.reviews = reviews\n",
    "        self.sentiment = sentiment\n",
    "        self.token_embedder = token_embedder\n",
    "        self.max_length = max_length\n",
    "        self.vector_size = vector_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        curr_review = self.reviews.iloc[index]\n",
    "        curr_review = curr_review.replace(',', '').replace('.', '').split()\n",
    "        curr_vect = []\n",
    "        count = 0\n",
    "        for word in curr_review:\n",
    "            if count >= self.max_length:\n",
    "                break\n",
    "            word_embedding = self.token_embedder.compute_embedding(word)\n",
    "            # Ensure the embedding matches the expected vector size\n",
    "            if word_embedding.shape[0] != self.vector_size:\n",
    "                # If not, you might need to truncate or pad the embedding\n",
    "                if word_embedding.shape[0] > self.vector_size:\n",
    "                    word_embedding = word_embedding[:self.vector_size]  # Truncate\n",
    "                else:\n",
    "                    # Pad the embedding with zeros if it's shorter than expected\n",
    "                    padding = np.zeros(self.vector_size - word_embedding.shape[0], dtype=float)\n",
    "                    word_embedding = np.concatenate((word_embedding, padding))\n",
    "            curr_vect.append(word_embedding)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "        # if review is less than max_length words, append zeros\n",
    "        while count < self.max_length:\n",
    "            curr_vect.append(np.zeros(self.vector_size, dtype=float))\n",
    "            count += 1\n",
    "        if len(curr_vect) == 0:\n",
    "            curr_vect = np.zeros([self.max_length, self.vector_size], dtype=float)\n",
    "        else:\n",
    "            curr_vect = np.array(curr_vect)\n",
    "        curr_vect = np.transpose(curr_vect)\n",
    "        # Convert to pytorch tensor\n",
    "        curr_vect = torch.from_numpy(curr_vect).float()\n",
    "        sentiment = self.sentiment.iloc[index]\n",
    "    \n",
    "        return curr_vect, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestReviewCNN(Dataset):\n",
    "    def __init__(self, reviews, sentiment, token_embedder, max_length=50, vector_size=300):\n",
    "        self.reviews = reviews\n",
    "        self.sentiment = sentiment\n",
    "        self.token_embedder = token_embedder\n",
    "        self.max_length = max_length\n",
    "        self.vector_size = vector_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        curr_review = self.reviews.iloc[index]\n",
    "        curr_review = curr_review.replace(',', '').replace('.', '').split()\n",
    "        curr_vect = []\n",
    "        count = 0\n",
    "        for word in curr_review:\n",
    "            if count >= self.max_length:\n",
    "                break\n",
    "            word_embedding = self.token_embedder.compute_embedding(word)\n",
    "            # Ensure the embedding matches the expected vector size\n",
    "            if word_embedding.shape[0] != self.vector_size:\n",
    "                # If not, you might need to truncate or pad the embedding\n",
    "                if word_embedding.shape[0] > self.vector_size:\n",
    "                    word_embedding = word_embedding[:self.vector_size]  # Truncate\n",
    "                else:\n",
    "                    # Pad the embedding with zeros if it's shorter than expected\n",
    "                    padding = np.zeros(self.vector_size - word_embedding.shape[0], dtype=float)\n",
    "                    word_embedding = np.concatenate((word_embedding, padding))\n",
    "            curr_vect.append(word_embedding)\n",
    "            count += 1\n",
    "\n",
    "        # if review is less than max_length words, append zeros\n",
    "        while count < self.max_length:\n",
    "            curr_vect.append(np.zeros(self.vector_size, dtype=float))\n",
    "            count += 1\n",
    "        if len(curr_vect) == 0:\n",
    "            curr_vect = np.zeros([self.max_length, self.vector_size], dtype=float)\n",
    "        else:\n",
    "            curr_vect = np.array(curr_vect)\n",
    "        curr_vect = np.transpose(curr_vect)\n",
    "        # Convert to pytorch tensor\n",
    "        curr_vect = torch.from_numpy(curr_vect).float()\n",
    "        sentiment = self.sentiment.iloc[index]\n",
    "    \n",
    "        return curr_vect, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cnn_custom_binary = TrainReviewCNN(X_train_raw_binary, Y_train_raw_binary, embedder)\n",
    "test_data_cnn_custom_binary = TestReviewCNN(X_test_raw_binary, Y_test_raw_binary, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "valid_size = 0.2\n",
    "\n",
    "num_train = len(train_data_cnn_custom_binary)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data_cnn_custom_binary, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data_cnn_custom_binary, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_data_cnn_custom_binary, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TernaryCNN(nn.Module):\n",
    "    def __init__(self, output_channels1=50, output_channels2=10, max_length=50, vector_size=300, num_classes=3):\n",
    "        super(TernaryCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=vector_size, out_channels=output_channels1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=output_channels1, out_channels=output_channels2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "\n",
    "        self.fc1 = nn.Linear(120, num_classes)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TernaryCNN(\n",
      "  (conv1): Conv1d(300, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(50, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=120, out_features=3, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (dropout2): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "CustomTernaryCNN = TernaryCNN().to(device)\n",
    "print(CustomTernaryCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(CustomTernaryCNN.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.885687 \tValidation Loss: 0.779584\n",
      "Validation loss decreased (inf --> 0.779584).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.762666 \tValidation Loss: 0.730206\n",
      "Validation loss decreased (0.779584 --> 0.730206).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.731783 \tValidation Loss: 0.717856\n",
      "Validation loss decreased (0.730206 --> 0.717856).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.713301 \tValidation Loss: 0.704750\n",
      "Validation loss decreased (0.717856 --> 0.704750).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.701194 \tValidation Loss: 0.685716\n",
      "Validation loss decreased (0.704750 --> 0.685716).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.692955 \tValidation Loss: 0.690633\n",
      "Epoch: 7 \tTraining Loss: 0.682242 \tValidation Loss: 0.679385\n",
      "Validation loss decreased (0.685716 --> 0.679385).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.677672 \tValidation Loss: 0.677982\n",
      "Validation loss decreased (0.679385 --> 0.677982).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.671703 \tValidation Loss: 0.672850\n",
      "Validation loss decreased (0.677982 --> 0.672850).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.667731 \tValidation Loss: 0.675388\n",
      "Epoch: 11 \tTraining Loss: 0.662890 \tValidation Loss: 0.668347\n",
      "Validation loss decreased (0.672850 --> 0.668347).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.659358 \tValidation Loss: 0.669981\n",
      "Epoch: 13 \tTraining Loss: 0.655637 \tValidation Loss: 0.668474\n",
      "Epoch: 14 \tTraining Loss: 0.652116 \tValidation Loss: 0.667886\n",
      "Validation loss decreased (0.668347 --> 0.667886).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.649005 \tValidation Loss: 0.674605\n",
      "Epoch: 16 \tTraining Loss: 0.646333 \tValidation Loss: 0.667496\n",
      "Validation loss decreased (0.667886 --> 0.667496).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.645449 \tValidation Loss: 0.666455\n",
      "Validation loss decreased (0.667496 --> 0.666455).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.642272 \tValidation Loss: 0.664510\n",
      "Validation loss decreased (0.666455 --> 0.664510).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.639830 \tValidation Loss: 0.663953\n",
      "Validation loss decreased (0.664510 --> 0.663953).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.638312 \tValidation Loss: 0.663307\n",
      "Validation loss decreased (0.663953 --> 0.663307).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.635088 \tValidation Loss: 0.665694\n",
      "Epoch: 22 \tTraining Loss: 0.634468 \tValidation Loss: 0.666910\n",
      "Epoch: 23 \tTraining Loss: 0.632948 \tValidation Loss: 0.664064\n",
      "Epoch: 24 \tTraining Loss: 0.630383 \tValidation Loss: 0.662873\n",
      "Validation loss decreased (0.663307 --> 0.662873).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.629009 \tValidation Loss: 0.677741\n",
      "Epoch: 26 \tTraining Loss: 0.627933 \tValidation Loss: 0.664451\n",
      "Epoch: 27 \tTraining Loss: 0.626019 \tValidation Loss: 0.663554\n",
      "Epoch: 28 \tTraining Loss: 0.624207 \tValidation Loss: 0.665622\n",
      "Epoch: 29 \tTraining Loss: 0.624049 \tValidation Loss: 0.665323\n",
      "Epoch: 30 \tTraining Loss: 0.622730 \tValidation Loss: 0.664855\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    CustomTernaryCNN.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.float()\n",
    "        data = data.to(device)\n",
    "        output = CustomTernaryCNN(data)\n",
    "        target = target.long()\n",
    "        target = target.to(device)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "    CustomTernaryCNN.eval()\n",
    "    for data, target in valid_loader:\n",
    "        data = data.float()\n",
    "        output = CustomTernaryCNN(data)\n",
    "        target = target.long()  # Convert target to torch.long\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
    "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch + 1,\n",
    "        train_loss,\n",
    "        valid_loss\n",
    "    ))\n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "        torch.save(CustomTernaryCNN.state_dict(), 'CustomTernaryCNN.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CNN using average custom Rolling hash vectors (Ternary) : 0.71704\n"
     ]
    }
   ],
   "source": [
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    actual_list = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.float()\n",
    "        # inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.append(int(predicted[0]))\n",
    "        actual_list.append(int(targets[0]))\n",
    "    total = 0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] == actual_list[i]:\n",
    "            total += 1\n",
    "    accuracy = float(total) / len(prediction_list)\n",
    "    return accuracy\n",
    "\n",
    "CustomTernaryCNN.load_state_dict(torch.load('CustomTernaryCNN.pt'))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data_cnn_custom_binary, batch_size=1)\n",
    "print('Accuracy of CNN using average custom Rolling hash vectors (Ternary) :',str(predict(CustomTernaryCNN, test_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
