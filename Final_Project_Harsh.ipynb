{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511777b5-b4f9-4f8e-9899-2b0ab94cce92",
   "metadata": {
    "id": "511777b5-b4f9-4f8e-9899-2b0ab94cce92"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c2d32a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39c2d32a",
    "outputId": "96a60952-532e-4396-d9a6-f3f857cf262d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "UUM83pBAoctF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUM83pBAoctF",
    "outputId": "31c91b16-0cbc-47bf-ea88-0fda5b37d438"
   },
   "outputs": [],
   "source": [
    "# Commenting for local execution\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "l1fOiPlfofJe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1fOiPlfofJe",
    "outputId": "db9db150-67ce-49ae-f15b-b8a2095af73e"
   },
   "outputs": [],
   "source": [
    "# Commenting for local execution\n",
    "# !unzip /content/drive/MyDrive/Project_Data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "uBFlLIdQnM-q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBFlLIdQnM-q",
    "outputId": "adc2838d-3b9a-40e9-ff85-30c77680ca3d"
   },
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb43c008-bdb6-495f-b2ed-ab3969613f14",
   "metadata": {
    "id": "bb43c008-bdb6-495f-b2ed-ab3969613f14"
   },
   "outputs": [],
   "source": [
    "class TokenEmbedder:\n",
    "    def __init__(self, d=768, N=3, B=int(1e9+7), random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the TokenEmbedder with given parameters.\n",
    "\n",
    "        :param d: Size of the embedding vector.\n",
    "        :param N: Number of i-grams to consider.\n",
    "        :param B: The modulus for the hash function and projection matrix normalization.\n",
    "        :param random_state: Seed for random number generation to ensure reproducibility.\n",
    "      `  \"\"\"\n",
    "        self.d = d\n",
    "        self.N = N\n",
    "        self.B = B\n",
    "        self.random_state = random_state\n",
    "        self.hash_seeds = self.initialize_hash_seeds()\n",
    "\n",
    "    def initialize_hash_seeds(self):\n",
    "        \"\"\"Initialize hash seeds with a fixed random state.\"\"\"\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        return rng.randint(low=1, high=np.iinfo(np.int32).max, size=self.d, dtype=np.int32)\n",
    "\n",
    "    def rolling_hash(self, text, i, base=256):\n",
    "        \"\"\"Compute rolling hash for all i-grams in a text.\"\"\"\n",
    "        h = 0\n",
    "        for x in range(i):\n",
    "            h = (h * base + ord(text[x])) % self.B\n",
    "        yield h\n",
    "        for x in range(len(text) - i):\n",
    "            h = (h * base - ord(text[x]) * pow(base, i, self.B) + ord(text[x + i])) % self.B\n",
    "            yield h\n",
    "\n",
    "    def compute_projection_matrix(self, s_i, h_i):\n",
    "        \"\"\"Compute and transform the projection matrix for i-grams.\"\"\"\n",
    "        P_i = np.outer(s_i, h_i) % self.B\n",
    "        P_i = P_i.astype(np.float64)  # Convert P_i to float64 before the division\n",
    "        P_i -= (P_i > self.B // 2) * self.B\n",
    "        P_i /= (self.B // 2)\n",
    "        return P_i\n",
    "\n",
    "    def compute_igram_embedding(self, P_i):\n",
    "        \"\"\"Compute the embedding for an i-gram by averaging the projection matrix.\"\"\"\n",
    "        return np.mean(P_i, axis=0)\n",
    "\n",
    "    def generate_token_embedding(self, embeddings):\n",
    "        \"\"\"Concatenate all i-gram embeddings to form the token embedding.\"\"\"\n",
    "        return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "    def compute_embedding(self, token):\n",
    "        \"\"\"\n",
    "        Compute the embedding for a given token by orchestrating the entire process.\n",
    "\n",
    "        :param token: The token for which to compute the embedding.\n",
    "        :return: The computed embedding vector for the token.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        l = len(token)\n",
    "        partitions = np.array_split(self.hash_seeds, self.N)  # Partitioning hash seeds for N i-grams\n",
    "\n",
    "        for i, h_i in enumerate(partitions, start=1):\n",
    "            if len(token) < i:  # Check if token is shorter than i\n",
    "                # Handle short tokens; options might include skipping or using a different approach\n",
    "                continue  # Skipping in this example\n",
    "\n",
    "            s_i = np.array(list(self.rolling_hash(token, i)))\n",
    "            P_i = self.compute_projection_matrix(s_i, h_i)\n",
    "            e_i = self.compute_igram_embedding(P_i)\n",
    "            embeddings.append(e_i)\n",
    "\n",
    "        if not embeddings:  # If no embeddings were generated (e.g., all tokens were too short)\n",
    "            return np.zeros(self.d)  # Return a zero vector of size d\n",
    "\n",
    "        return self.generate_token_embedding(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b580f8-fd95-44d4-949c-6b395014ddd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5b580f8-fd95-44d4-949c-6b395014ddd9",
    "outputId": "6d968123-fa07-44c5-b816-839ea86394fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/krt4vhn17ls6fxrd79rqftk00000gn/T/ipykernel_15397/821680044.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('amazon_reviews_us_Office_Products_v1_00.tsv',on_bad_lines='skip', sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('amazon_reviews_us_Office_Products_v1_00.tsv',on_bad_lines='skip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee1dd74-14be-477a-b885-ce9e7a9b0747",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ee1dd74-14be-477a-b885-ce9e7a9b0747",
    "outputId": "562fbaf7-16b0-4ab2-84da-488463f97acd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews with Rating > 3: 2001122\n",
      "Number of Reviews with Rating <= 2: 445349\n",
      "Number of Reviews with Rating = 3: 193686\n"
     ]
    }
   ],
   "source": [
    "# keep only reviews and ratings\n",
    "df = df[['star_rating', 'review_body']]\n",
    "\n",
    "# Check for null values in the df\n",
    "df.isnull().any(axis=1).sum()\n",
    "df = df.dropna()\n",
    "\n",
    "# it seems that some values of star_rating are string while some are numeric. the below code will give an error and hence i was able to deduce this\n",
    "# df['sentiment'] = df['star_rating'].map(lambda x: 1 if x > 3 else 0 if x <= 2 else None)\n",
    "# df.shape\n",
    "\n",
    "# Convert 'star_rating' to numeric\n",
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "df['star_rating'] = df['star_rating'].astype(int)\n",
    "\n",
    "# Get counts of reviews for each sentiment class\n",
    "reviews_greater_than_3 = df[df['star_rating'] > 3].shape[0]\n",
    "reviews_less_than_equal_2 = df[df['star_rating'] <= 2].shape[0]\n",
    "reviews_equal_3 = df[df['star_rating'] == 3].shape[0]\n",
    "\n",
    "print(\"Number of Reviews with Rating > 3:\", reviews_greater_than_3)\n",
    "print(\"Number of Reviews with Rating <= 2:\", reviews_less_than_equal_2)\n",
    "print(\"Number of Reviews with Rating = 3:\", reviews_equal_3)\n",
    "\n",
    "# create sentiment column\n",
    "df['sentiment'] = df['star_rating'].map(lambda x: 0 if x > 3 else 1 if x <= 2 else 2 if x == 3 else None)\n",
    "\n",
    "\n",
    "# convert sentiment to int type\n",
    "df['sentiment'] = df['sentiment'].astype(int)\n",
    "\n",
    "rating_one = df[df['star_rating'] == 1].sample(n=50000, random_state=42)\n",
    "rating_two = df[df['star_rating'] == 2].sample(n=50000, random_state=42)\n",
    "rating_three = df[df['star_rating'] == 3].sample(n=50000, random_state=42)\n",
    "rating_four = df[df['star_rating'] == 4].sample(n=50000, random_state=42)\n",
    "rating_five = df[df['star_rating'] == 5].sample(n=50000, random_state=42)\n",
    "\n",
    "downsized_df = pd.concat([rating_one, rating_two, rating_three, rating_four, rating_five])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a09486f-8bae-49bf-9e00-f501cd94a5d6",
   "metadata": {
    "id": "5a09486f-8bae-49bf-9e00-f501cd94a5d6"
   },
   "outputs": [],
   "source": [
    "contractions = {\"ain't\": 'am not / is not / are not / has not / have not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would / he had', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he will have', \"he's\": 'he is / he has', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would / I had', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"isn't\": 'is not', \"it'd\": 'it would / it had', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is / it has', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would / she had', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is / she has', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so is', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is / that has', \"there'd\": 'there had', \"there'd've\": 'there would have', \"there's\": 'there is / there has', \"they'd\": 'they would / they had', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would / we had', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is / what has', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is / where has', \"where've\": 'where have', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is / who has', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'alls\": 'you alls', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would / you had', \"you'd've\": 'you would have', \"you'll\": 'you you will', \"you'll've\": 'you you will have', \"you're\": 'you are', \"you've\": 'you have', \"who'd\": 'who would / who had', \"who're\": 'who are'}\n",
    "\n",
    "def expand_contractions(text):\n",
    "     for contraction, expansion_options in contractions.items():\n",
    "        # Select the first option when there are multiple choices\n",
    "        first_option = expansion_options.split('/')[0].strip()\n",
    "        text = text.replace(contraction, first_option)\n",
    "     return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d4e9868-440c-4b5b-9015-4cae62ea1352",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d4e9868-440c-4b5b-9015-4cae62ea1352",
    "outputId": "6da17c6a-22e0-4ced-f1ca-71865a2ab2ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/krt4vhn17ls6fxrd79rqftk00000gn/T/ipykernel_15397/803437059.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n",
      "/var/folders/9p/krt4vhn17ls6fxrd79rqftk00000gn/T/ipykernel_15397/803437059.py:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n"
     ]
    }
   ],
   "source": [
    "downsized_df['review_body'] = downsized_df['review_body'].str.lower()\n",
    "downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n",
    "downsized_df['review_body'] = downsized_df['review_body'].str.replace('http[s]?://\\S+', '', regex=True)\n",
    "downsized_df['review_body'] = downsized_df['review_body'].str.replace(r'[^a-zA-Z ]', '', regex=True)\n",
    "downsized_df['review_body'] = downsized_df['review_body'].str.replace(' +', ' ', regex=True)\n",
    "downsized_df['review_body'] = downsized_df['review_body'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b619a9-33ed-4d8e-a98d-d7d664253fab",
   "metadata": {
    "id": "33b619a9-33ed-4d8e-a98d-d7d664253fab"
   },
   "source": [
    "# HashBased embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ac7282-7cc6-4de5-a961-bb5ef1e2f872",
   "metadata": {
    "id": "06ac7282-7cc6-4de5-a961-bb5ef1e2f872"
   },
   "outputs": [],
   "source": [
    "# simple_df = downsized_df[downsized_df['sentiment'].isin([0, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ced01bc-88ad-4584-be06-7f457215f9bf",
   "metadata": {
    "id": "4ced01bc-88ad-4584-be06-7f457215f9bf"
   },
   "outputs": [],
   "source": [
    "embedder = TokenEmbedder(d=300, N=3, B=int(1e9+7), random_state=42) # Adjust the dimensions and parameters as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f5314c2-76cf-483d-8e33-d9bcf30ff12b",
   "metadata": {
    "id": "6f5314c2-76cf-483d-8e33-d9bcf30ff12b"
   },
   "outputs": [],
   "source": [
    "def create_X_avg_custom_with_embedder(df, embedder):\n",
    "    X_avg = []\n",
    "\n",
    "    for review in df['review_body']:\n",
    "        # Tokenize the review text\n",
    "        curr_review = review.replace(',', '').replace('.', '').split()\n",
    "        embeddings = []\n",
    "\n",
    "        for token in curr_review:\n",
    "            # Generate an embedding for each token and ensure it's properly shaped\n",
    "            token_embedding = embedder.compute_embedding(token)\n",
    "            if token_embedding.shape[0] == embedder.d:  # Check if embedding has expected length\n",
    "                embeddings.append(token_embedding)\n",
    "\n",
    "        # Ensure embeddings is a 2D array with consistent inner dimension\n",
    "        if len(embeddings) > 0:\n",
    "            embeddings = np.vstack(embeddings)  # Stack embeddings vertically to create a 2D array\n",
    "            # Average the embeddings for the review\n",
    "            review_embedding = np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            # If no valid embeddings were generated, use a zero vector\n",
    "            review_embedding = np.zeros(embedder.d)\n",
    "\n",
    "        X_avg.append(review_embedding)\n",
    "\n",
    "    return np.array(X_avg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "164ae12b-d74d-431c-bb19-7b2c3e419c4d",
   "metadata": {
    "id": "164ae12b-d74d-431c-bb19-7b2c3e419c4d"
   },
   "outputs": [],
   "source": [
    "X_avg_custom = create_X_avg_custom_with_embedder(downsized_df, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d67ffbce-440c-408e-a8f7-92d6837841e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d67ffbce-440c-408e-a8f7-92d6837841e8",
    "outputId": "25ff3086-da78-4a4d-8cd9-ee71d33d81a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_avg_custom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89878087-d6d3-4424-99ec-ef8b07985704",
   "metadata": {
    "id": "89878087-d6d3-4424-99ec-ef8b07985704"
   },
   "outputs": [],
   "source": [
    "X_train_custom, X_test_custom, Y_train_custom, Y_test_custom = train_test_split(X_avg_custom, downsized_df['sentiment'], test_size=0.2, random_state=48)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcecfd97-fe83-4712-b427-9aa9730b6c1a",
   "metadata": {
    "id": "bcecfd97-fe83-4712-b427-9aa9730b6c1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_avg_custom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae9c33b7-eb71-41a8-afc1-c61e94136c64",
   "metadata": {
    "id": "ae9c33b7-eb71-41a8-afc1-c61e94136c64"
   },
   "outputs": [],
   "source": [
    "X_train_raw_ternary, X_test_raw_ternary, Y_train_raw_ternary, Y_test_raw_ternary = train_test_split(downsized_df['review_body'], downsized_df['sentiment'], test_size=0.2, random_state=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41f22ca9-de9c-43e3-a75a-4c2b8a7bff3a",
   "metadata": {
    "id": "41f22ca9-de9c-43e3-a75a-4c2b8a7bff3a"
   },
   "outputs": [],
   "source": [
    "class TrainReview(Dataset):\n",
    "    def __init__(self, reviews, sentiment, token_embedder):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with reviews, sentiment labels, and a token embedder.\n",
    "\n",
    "        :param reviews: A pandas Series or DataFrame column containing review texts.\n",
    "        :param sentiment: A pandas Series or DataFrame column containing sentiment labels.\n",
    "        :param token_embedder: An instance of the TokenEmbedder class.\n",
    "        \"\"\"\n",
    "        self.reviews = reviews\n",
    "        self.sentiment = sentiment\n",
    "        self.token_embedder = token_embedder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        curr_review = self.reviews.iloc[index]\n",
    "        curr_review = curr_review.replace(',', '').replace('.', '').split()\n",
    "        curr_vect = []\n",
    "\n",
    "        for word in curr_review:\n",
    "            # Use the TokenEmbedder's compute_embedding method to get the embedding\n",
    "            word_embedding = self.token_embedder.compute_embedding(word)\n",
    "            if word_embedding.shape[0] == self.token_embedder.d:\n",
    "                curr_vect.append(word_embedding)\n",
    "        if len(curr_vect) == 0:\n",
    "            curr_vect = np.zeros(self.token_embedder.d, dtype=float)  # Use the embedder's dimension\n",
    "        else:\n",
    "            curr_vect = np.mean(np.array(curr_vect), axis=0)\n",
    "\n",
    "        # Convert to pytorch tensor\n",
    "        curr_vect = torch.from_numpy(curr_vect).float()\n",
    "        sentiment = self.sentiment.iloc[index]\n",
    "\n",
    "        return curr_vect, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "463780c9-011a-4bb4-a41b-4ab4a7e65af7",
   "metadata": {
    "id": "463780c9-011a-4bb4-a41b-4ab4a7e65af7"
   },
   "outputs": [],
   "source": [
    "class TestReview(Dataset):\n",
    "    def __init__(self, reviews, sentiment, token_embedder):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with reviews, sentiment labels, and a token embedder.\n",
    "\n",
    "        :param reviews: A pandas Series or DataFrame column containing review texts.\n",
    "        :param sentiment: A pandas Series or DataFrame column containing sentiment labels.\n",
    "        :param token_embedder: An instance of the TokenEmbedder class for generating embeddings.\n",
    "        \"\"\"\n",
    "        self.reviews = reviews\n",
    "        self.sentiment = sentiment\n",
    "        self.token_embedder = token_embedder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        curr_review = self.reviews.iloc[index]\n",
    "        curr_review = curr_review.replace(',', '').replace('.', '').split()\n",
    "        curr_vect = []\n",
    "\n",
    "        for word in curr_review:\n",
    "            # Use the TokenEmbedder's compute_embedding method to get the embedding for each word\n",
    "            word_embedding = self.token_embedder.compute_embedding(word)\n",
    "            if word_embedding.shape[0] == self.token_embedder.d:\n",
    "\n",
    "                curr_vect.append(word_embedding)\n",
    "\n",
    "        if len(curr_vect) == 0:\n",
    "            curr_vect = np.zeros(self.token_embedder.d, dtype=float)  # Use the embedder's dimension\n",
    "        else:\n",
    "            curr_vect = np.mean(np.array(curr_vect), axis=0)\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        curr_vect = torch.from_numpy(curr_vect).float()\n",
    "        sentiment = self.sentiment.iloc[index]\n",
    "\n",
    "        return curr_vect, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "409b4474-0633-4c6c-aa56-0b84dcfbe87b",
   "metadata": {
    "id": "409b4474-0633-4c6c-aa56-0b84dcfbe87b"
   },
   "outputs": [],
   "source": [
    "# Assuming token_embedder is an instance of TokenEmbedder\n",
    "train_data_avg_custom_ternary = TrainReview(X_train_raw_ternary, Y_train_raw_ternary, embedder)\n",
    "test_data_avg_custom_ternary = TestReview(X_test_raw_ternary, Y_test_raw_ternary, embedder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aefb688-6626-4dbc-85c5-4bd3cd520157",
   "metadata": {
    "id": "1aefb688-6626-4dbc-85c5-4bd3cd520157"
   },
   "outputs": [],
   "source": [
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_avg_custom_ternary)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data_avg_custom_ternary, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data_avg_custom_ternary, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_data_avg_custom_ternary, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b459552-98c7-4278-97b5-bb6824a735ad",
   "metadata": {
    "id": "6b459552-98c7-4278-97b5-bb6824a735ad"
   },
   "outputs": [],
   "source": [
    "class FFNetTernary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFNetTernary, self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Flatten the input if it's not already flattened\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        # Apply the first linear layer with activation and dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply the second linear layer with activation and dropout\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output layer with two units (ternary classification)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9af2b0a5-7514-4a46-81a2-93b3494a5c46",
   "metadata": {
    "id": "9af2b0a5-7514-4a46-81a2-93b3494a5c46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNetTernary(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "FFNetCustomTernary = FFNetTernary()\n",
    "FFNetCustomTernary.to(device)\n",
    "print(FFNetCustomTernary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ea3b83d-1515-4f05-a4ea-4233966fe158",
   "metadata": {
    "id": "0ea3b83d-1515-4f05-a4ea-4233966fe158"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(FFNetCustomTernary.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcb9639a-dbbd-472d-a29c-fe261832eb72",
   "metadata": {
    "id": "dcb9639a-dbbd-472d-a29c-fe261832eb72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.082549 \tValidation Loss: 1.069310\n",
      "Validation loss decreased (inf --> 1.069310).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.064024 \tValidation Loss: 1.059808\n",
      "Validation loss decreased (1.069310 --> 1.059808).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.058400 \tValidation Loss: 1.056487\n",
      "Validation loss decreased (1.059808 --> 1.056487).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.056316 \tValidation Loss: 1.055167\n",
      "Validation loss decreased (1.056487 --> 1.055167).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.055471 \tValidation Loss: 1.054511\n",
      "Validation loss decreased (1.055167 --> 1.054511).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.054944 \tValidation Loss: 1.054072\n",
      "Validation loss decreased (1.054511 --> 1.054072).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.054539 \tValidation Loss: 1.053686\n",
      "Validation loss decreased (1.054072 --> 1.053686).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.054210 \tValidation Loss: 1.053306\n",
      "Validation loss decreased (1.053686 --> 1.053306).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.053779 \tValidation Loss: 1.052870\n",
      "Validation loss decreased (1.053306 --> 1.052870).  Saving model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     10\u001b[0m FFNetCustomTernary\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# prep model for training\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m, in \u001b[0;36mTrainReview.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     20\u001b[0m curr_vect \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m curr_review:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Use the TokenEmbedder's compute_embedding method to get the embedding\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     word_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedder\u001b[38;5;241m.\u001b[39mcompute_embedding(word)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word_embedding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedder\u001b[38;5;241m.\u001b[39md:\n\u001b[1;32m     26\u001b[0m         curr_vect\u001b[38;5;241m.\u001b[39mappend(word_embedding)\n",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m, in \u001b[0;36mTokenEmbedder.compute_embedding\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     64\u001b[0m     s_i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrolling_hash(token, i)))\n\u001b[1;32m     65\u001b[0m     P_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_projection_matrix(s_i, h_i)\n\u001b[0;32m---> 66\u001b[0m     e_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_igram_embedding(P_i)\n\u001b[1;32m     67\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(e_i)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m embeddings:  \u001b[38;5;66;03m# If no embeddings were generated (e.g., all tokens were too short)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m, in \u001b[0;36mTokenEmbedder.compute_igram_embedding\u001b[0;34m(self, P_i)\u001b[0m\n\u001b[1;32m     37\u001b[0m     P_i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m P_i\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_igram_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, P_i):\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the embedding for an i-gram by averaging the projection matrix.\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(P_i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    FFNetCustomTernary.train()  # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.float()\n",
    "        data = data.to(device)\n",
    "        output = FFNetCustomTernary(data)\n",
    "        target = target.long()  # Convert target to torch.long\n",
    "        output = output.to(device)\n",
    "        target = target.to(device)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "    FFNetCustomTernary.eval()  # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        data = data.float()\n",
    "        data = data.to(device)\n",
    "        output = FFNetCustomTernary(data)\n",
    "        target = target.long()  # Convert target to torch.long\n",
    "        output = output.to(device)\n",
    "        target = target.to(device)\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
    "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch + 1,\n",
    "        train_loss,\n",
    "        valid_loss\n",
    "    ))\n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "        torch.save(FFNetCustomTernary.state_dict(), 'FFNetCustomRollingHashTernary.pt')\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7e90c-84be-4e56-aedb-a1ac31a4fc3a",
   "metadata": {
    "id": "8aa7e90c-84be-4e56-aedb-a1ac31a4fc3a"
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    actual_list = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.float()\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.append(int(predicted[0]))\n",
    "        actual_list.append(int(targets[0]))\n",
    "    total = 0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] == actual_list[i]:\n",
    "            total += 1\n",
    "    accuracy = float(total) / len(prediction_list)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673f966d-99f5-4161-91fa-d58afe3cada7",
   "metadata": {
    "id": "673f966d-99f5-4161-91fa-d58afe3cada7"
   },
   "outputs": [],
   "source": [
    "FFNetCustomTernary.load_state_dict(torch.load('FFNetCustomRollingHashTernary.pt'))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data_avg_custom_ternary, batch_size=1)\n",
    "print('Accuracy of FNN using average custom Rolling hash vectors (ternary) :',str(predict(FFNetCustomTernary, test_loader)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
