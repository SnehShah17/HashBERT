{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedder:\n",
    "    def __init__(self, d=768, N=3, B=int(1e9+7), random_state=42):\n",
    "        self.d = d\n",
    "        self.N = N\n",
    "        self.B = B\n",
    "        self.random_state = random_state\n",
    "        self.hash_seeds = self.initialize_hash_seeds()\n",
    "        self.embedding_dict = {}\n",
    "\n",
    "    def initialize_hash_seeds(self):\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        return rng.randint(low=1, high=np.iinfo(np.int32).max, size=self.d, dtype=np.int32)\n",
    "\n",
    "    def rolling_hash(self, text, i, base=256):\n",
    "        h = 0\n",
    "        for x in range(i):\n",
    "            h = (h * base + ord(text[x])) % self.B\n",
    "        yield h\n",
    "        for x in range(len(text) - i):\n",
    "            h = (h * base - ord(text[x]) * pow(base, i, self.B) + ord(text[x + i])) % self.B\n",
    "            yield h\n",
    "\n",
    "    def compute_projection_matrix(self, s_i, h_i):\n",
    "        P_i = np.outer(s_i, h_i) % self.B\n",
    "        P_i = P_i.astype(np.float64)\n",
    "        P_i -= (P_i > self.B // 2) * self.B\n",
    "        P_i /= (self.B // 2)\n",
    "        return P_i\n",
    "\n",
    "    def compute_igram_embedding(self, P_i):\n",
    "        return np.mean(P_i, axis=0)\n",
    "\n",
    "    def generate_token_embedding(self, embeddings):\n",
    "        token_embedding = np.concatenate(embeddings, axis=0)\n",
    "        return token_embedding\n",
    "\n",
    "    def compute_embedding(self, token):\n",
    "        if token not in self.embedding_dict:\n",
    "            embeddings = []\n",
    "            l = len(token)\n",
    "            partitions = np.array_split(self.hash_seeds, self.N)  # Partitioning hash seeds for N i-grams\n",
    "\n",
    "            for i, h_i in enumerate(partitions, start=1):\n",
    "                if len(token) < i:  \n",
    "                    continue \n",
    "\n",
    "                s_i = np.array(list(self.rolling_hash(token, i)))\n",
    "                P_i = self.compute_projection_matrix(s_i, h_i)\n",
    "                e_i = self.compute_igram_embedding(P_i)\n",
    "                embeddings.append(e_i)\n",
    "\n",
    "            if not embeddings:  \n",
    "                return np.zeros(self.d) \n",
    "\n",
    "            token_embedding = self.generate_token_embedding(embeddings)\n",
    "            self.embedding_dict[token] = token_embedding\n",
    "            return token_embedding\n",
    "        else:\n",
    "            return self.embedding_dict[token]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/krt4vhn17ls6fxrd79rqftk00000gn/T/ipykernel_66214/821680044.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('amazon_reviews_us_Office_Products_v1_00.tsv',on_bad_lines='skip', sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('amazon_reviews_us_Office_Products_v1_00.tsv',on_bad_lines='skip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>52782374</td>\n",
       "      <td>R1PR37BR7G3M6A</td>\n",
       "      <td>B00D7H8XB6</td>\n",
       "      <td>868449945</td>\n",
       "      <td>AmazonBasics 12-Sheet High-Security Micro-Cut ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>24045652</td>\n",
       "      <td>R3BDDDZMZBZDPU</td>\n",
       "      <td>B001XCWP34</td>\n",
       "      <td>33521401</td>\n",
       "      <td>Derwent Colored Pencils, Inktense Ink Pencils,...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>US</td>\n",
       "      <td>53005790</td>\n",
       "      <td>RLI7EI10S7SN0</td>\n",
       "      <td>B00000DM9M</td>\n",
       "      <td>223408988</td>\n",
       "      <td>PalmOne III Leather Belt Clip Case</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Great value! A must if you hate to carry thing...</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "      <td>1998-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>US</td>\n",
       "      <td>52188548</td>\n",
       "      <td>R1F3SRK9MHE6A3</td>\n",
       "      <td>B00000DM9M</td>\n",
       "      <td>223408988</td>\n",
       "      <td>PalmOne III Leather Belt Clip Case</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Attaches the Palm Pilot like an appendage</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "      <td>1998-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>US</td>\n",
       "      <td>52090046</td>\n",
       "      <td>R23V0C4NRJL8EM</td>\n",
       "      <td>0807865001</td>\n",
       "      <td>307284585</td>\n",
       "      <td>Gods and Heroes of Ancient Greece</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Excellent information, pictures and stories, I...</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "      <td>1998-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>US</td>\n",
       "      <td>52503173</td>\n",
       "      <td>R13ZAE1ATEUC1T</td>\n",
       "      <td>1572313188</td>\n",
       "      <td>870359649</td>\n",
       "      <td>Microsoft EXCEL 97/ Visual Basic Step-by-Step ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>class text</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "      <td>1998-08-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>US</td>\n",
       "      <td>52585611</td>\n",
       "      <td>RE8J5O2GY04NN</td>\n",
       "      <td>1572313188</td>\n",
       "      <td>870359649</td>\n",
       "      <td>Microsoft EXCEL 97/ Visual Basic Step-by-Step ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Microsoft's Finest</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "      <td>1998-07-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640254 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0                US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1                US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2                US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "3                US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
       "4                US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
       "...             ...          ...             ...         ...             ...   \n",
       "2640249          US     53005790   RLI7EI10S7SN0  B00000DM9M       223408988   \n",
       "2640250          US     52188548  R1F3SRK9MHE6A3  B00000DM9M       223408988   \n",
       "2640251          US     52090046  R23V0C4NRJL8EM  0807865001       307284585   \n",
       "2640252          US     52503173  R13ZAE1ATEUC1T  1572313188       870359649   \n",
       "2640253          US     52585611   RE8J5O2GY04NN  1572313188       870359649   \n",
       "\n",
       "                                             product_title product_category  \\\n",
       "0           Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1                Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2        Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "3        AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
       "4        Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
       "...                                                    ...              ...   \n",
       "2640249                 PalmOne III Leather Belt Clip Case  Office Products   \n",
       "2640250                 PalmOne III Leather Belt Clip Case  Office Products   \n",
       "2640251                  Gods and Heroes of Ancient Greece  Office Products   \n",
       "2640252  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
       "2640253  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
       "\n",
       "        star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0                 5            0.0          0.0    N                 Y   \n",
       "1                 5            0.0          1.0    N                 Y   \n",
       "2                 5            0.0          0.0    N                 Y   \n",
       "3                 1            2.0          3.0    N                 Y   \n",
       "4                 4            0.0          0.0    N                 Y   \n",
       "...             ...            ...          ...  ...               ...   \n",
       "2640249           4           26.0         26.0    N                 N   \n",
       "2640250           4           18.0         18.0    N                 N   \n",
       "2640251           4            9.0         16.0    N                 N   \n",
       "2640252           5            0.0          0.0    N                 N   \n",
       "2640253           5            0.0          0.0    N                 N   \n",
       "\n",
       "                                           review_headline  \\\n",
       "0                                               Five Stars   \n",
       "1        Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                            but I am sure I will like it.   \n",
       "3        and the shredder was dirty and the bin was par...   \n",
       "4                                               Four Stars   \n",
       "...                                                    ...   \n",
       "2640249  Great value! A must if you hate to carry thing...   \n",
       "2640250          Attaches the Palm Pilot like an appendage   \n",
       "2640251  Excellent information, pictures and stories, I...   \n",
       "2640252                                         class text   \n",
       "2640253                                 Microsoft's Finest   \n",
       "\n",
       "                                               review_body review_date  \n",
       "0                                           Great product.  2015-08-31  \n",
       "1        What's to say about this commodity item except...  2015-08-31  \n",
       "2          Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
       "3        Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
       "4                          Gorgeous colors and easy to use  2015-08-31  \n",
       "...                                                    ...         ...  \n",
       "2640249  I can't live anymore whithout my Palm III. But...  1998-12-07  \n",
       "2640250  Although the Palm Pilot is thin and compact it...  1998-11-30  \n",
       "2640251  This book had a lot of great content without b...  1998-10-15  \n",
       "2640252  I am teaching a course in Excel and am using t...  1998-08-22  \n",
       "2640253  A very comprehensive layout of exactly how Vis...  1998-07-15  \n",
       "\n",
       "[2640254 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews with Rating > 3: 2001122\n",
      "Number of Reviews with Rating <= 2: 445349\n",
      "Number of Reviews with Rating = 3: 193686\n"
     ]
    }
   ],
   "source": [
    "# keep only reviews and ratings\n",
    "df = df[['star_rating', 'review_body']]\n",
    "\n",
    "# Check for null values in the df\n",
    "df.isnull().any(axis=1).sum()\n",
    "df = df.dropna()\n",
    "\n",
    "# it seems that some values of star_rating are string while some are numeric. the below code will give an error and hence i was able to deduce this\n",
    "# df['sentiment'] = df['star_rating'].map(lambda x: 1 if x > 3 else 0 if x <= 2 else None)\n",
    "# df.shape\n",
    "\n",
    "# Convert 'star_rating' to numeric\n",
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "df['star_rating'] = df['star_rating'].astype(int)\n",
    "\n",
    "# Get counts of reviews for each sentiment class\n",
    "reviews_greater_than_3 = df[df['star_rating'] > 3].shape[0]\n",
    "reviews_less_than_equal_2 = df[df['star_rating'] <= 2].shape[0]\n",
    "reviews_equal_3 = df[df['star_rating'] == 3].shape[0]\n",
    "\n",
    "print(\"Number of Reviews with Rating > 3:\", reviews_greater_than_3)\n",
    "print(\"Number of Reviews with Rating <= 2:\", reviews_less_than_equal_2)\n",
    "print(\"Number of Reviews with Rating = 3:\", reviews_equal_3)\n",
    "\n",
    "# create sentiment column\n",
    "df['sentiment'] = df['star_rating'].map(lambda x: 0 if x > 3 else 1 if x <= 2 else 2 if x == 3 else None)\n",
    "\n",
    "\n",
    "# convert sentiment to int type\n",
    "df['sentiment'] = df['sentiment'].astype(int)\n",
    "\n",
    "rating_one = df[df['star_rating'] == 1].sample(n=50000, random_state=42)\n",
    "rating_two = df[df['star_rating'] == 2].sample(n=50000, random_state=42)\n",
    "rating_three = df[df['star_rating'] == 3].sample(n=50000, random_state=42)\n",
    "rating_four = df[df['star_rating'] == 4].sample(n=50000, random_state=42)\n",
    "rating_five = df[df['star_rating'] == 5].sample(n=50000, random_state=42)\n",
    "\n",
    "downsized_df = pd.concat([rating_one, rating_two, rating_three, rating_four, rating_five])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\"ain't\": 'am not / is not / are not / has not / have not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would / he had', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he will have', \"he's\": 'he is / he has', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would / I had', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"isn't\": 'is not', \"it'd\": 'it would / it had', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is / it has', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would / she had', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is / she has', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so is', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is / that has', \"there'd\": 'there had', \"there'd've\": 'there would have', \"there's\": 'there is / there has', \"they'd\": 'they would / they had', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would / we had', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is / what has', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is / where has', \"where've\": 'where have', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is / who has', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'alls\": 'you alls', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would / you had', \"you'd've\": 'you would have', \"you'll\": 'you you will', \"you'll've\": 'you you will have', \"you're\": 'you are', \"you've\": 'you have', \"who'd\": 'who would / who had', \"who're\": 'who are'}\n",
    "\n",
    "def expand_contractions(text):\n",
    "     for contraction, expansion_options in contractions.items():\n",
    "        # Select the first option when there are multiple choices\n",
    "        first_option = expansion_options.split('/')[0].strip()\n",
    "        text = text.replace(contraction, first_option)\n",
    "     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/krt4vhn17ls6fxrd79rqftk00000gn/T/ipykernel_66214/803437059.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n",
      "/var/folders/9p/krt4vhn17ls6fxrd79rqftk00000gn/T/ipykernel_66214/803437059.py:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n"
     ]
    }
   ],
   "source": [
    "downsized_df['review_body'] = downsized_df['review_body'].str.lower()\n",
    "downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n",
    "downsized_df['review_body'] = downsized_df['review_body'].str.replace('http[s]?://\\S+', '', regex=True)\n",
    "downsized_df['review_body'] = downsized_df['review_body'].str.replace(r'[^a-zA-Z ]', '', regex=True)\n",
    "downsized_df['review_body'] = downsized_df['review_body'].str.replace(' +', ' ', regex=True)\n",
    "downsized_df['review_body'] = downsized_df['review_body'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2520757</th>\n",
       "      <td>1</td>\n",
       "      <td>i bought this thinking it would be a good alte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800253</th>\n",
       "      <td>1</td>\n",
       "      <td>i have used transfers many times in the past w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366081</th>\n",
       "      <td>1</td>\n",
       "      <td>sorry to report that this ink cartridge spille...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159817</th>\n",
       "      <td>1</td>\n",
       "      <td>doesnt work dont buy i tried two times at inst...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118644</th>\n",
       "      <td>1</td>\n",
       "      <td>i replaced the original canon toner cartridge ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840862</th>\n",
       "      <td>5</td>\n",
       "      <td>a unique gift that helps save those amazing cr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281476</th>\n",
       "      <td>5</td>\n",
       "      <td>exactly as described i got two of these one in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804044</th>\n",
       "      <td>5</td>\n",
       "      <td>these are much better than some other brands i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221856</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent supplier just as promised and timely...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564798</th>\n",
       "      <td>5</td>\n",
       "      <td>good product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  \\\n",
       "2520757            1  i bought this thinking it would be a good alte...   \n",
       "800253             1  i have used transfers many times in the past w...   \n",
       "2366081            1  sorry to report that this ink cartridge spille...   \n",
       "1159817            1  doesnt work dont buy i tried two times at inst...   \n",
       "2118644            1  i replaced the original canon toner cartridge ...   \n",
       "...              ...                                                ...   \n",
       "840862             5  a unique gift that helps save those amazing cr...   \n",
       "1281476            5  exactly as described i got two of these one in...   \n",
       "804044             5  these are much better than some other brands i...   \n",
       "221856             5  excellent supplier just as promised and timely...   \n",
       "564798             5                                       good product   \n",
       "\n",
       "         sentiment  \n",
       "2520757          1  \n",
       "800253           1  \n",
       "2366081          1  \n",
       "1159817          1  \n",
       "2118644          1  \n",
       "...            ...  \n",
       "840862           0  \n",
       "1281476          0  \n",
       "804044           0  \n",
       "221856           0  \n",
       "564798           0  \n",
       "\n",
       "[250000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsized_df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = TokenEmbedder(d=300, N=3, B=int(1e9+7), random_state=42) # Adjust the dimensions and parameters as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_X_avg_custom_with_embedder(df, embedder):\n",
    "#     X_avg = []\n",
    "\n",
    "#     for review in df['review_body']:\n",
    "#         # Tokenize the review text\n",
    "#         curr_review = review.replace(',', '').replace('.', '').split()\n",
    "#         embeddings = []\n",
    "\n",
    "#         for token in curr_review:\n",
    "#             # Generate an embedding for each token and ensure it's properly shaped\n",
    "#             token_embedding = embedder.compute_embedding(token)\n",
    "#             if token_embedding.shape[0] == embedder.d:  # Check if embedding has expected length\n",
    "#                 embeddings.append(token_embedding)\n",
    "\n",
    "#         # Ensure embeddings is a 2D array with consistent inner dimension\n",
    "#         if len(embeddings) > 0:\n",
    "#             embeddings = np.vstack(embeddings)  # Stack embeddings vertically to create a 2D array\n",
    "#             # Average the embeddings for the review\n",
    "#             review_embedding = np.mean(embeddings, axis=0)\n",
    "#         else:\n",
    "#             # If no valid embeddings were generated, use a zero vector\n",
    "#             review_embedding = np.zeros(embedder.d)\n",
    "\n",
    "#         X_avg.append(review_embedding)\n",
    "\n",
    "#     return np.array(X_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_avg_custom = create_X_avg_custom_with_embedder(downsized_df, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_avg_custom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_custom, X_test_custom, Y_train_custom, Y_test_custom = train_test_split(X_avg_custom, downsized_df['sentiment'], test_size=0.2, random_state=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw_binary, X_test_raw_binary, Y_train_raw_binary, Y_test_raw_binary = train_test_split(downsized_df['review_body'], downsized_df['sentiment'], test_size=0.2, random_state=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainReview(Dataset):\n",
    "#     def __init__(self, reviews, sentiment, token_embedder):\n",
    "#         \"\"\"\n",
    "#         Initializes the dataset with reviews, sentiment labels, and a token embedder.\n",
    "        \n",
    "#         :param reviews: A pandas Series or DataFrame column containing review texts.\n",
    "#         :param sentiment: A pandas Series or DataFrame column containing sentiment labels.\n",
    "#         :param token_embedder: An instance of the TokenEmbedder class.\n",
    "#         \"\"\"\n",
    "#         self.reviews = reviews\n",
    "#         self.sentiment = sentiment\n",
    "#         self.token_embedder = token_embedder\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.reviews)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         curr_review = self.reviews.iloc[index]\n",
    "#         curr_review = curr_review.replace(',', '').replace('.', '').split()\n",
    "#         curr_vect = []\n",
    "\n",
    "#         for word in curr_review:\n",
    "#             # Use the TokenEmbedder's compute_embedding method to get the embedding\n",
    "#             word_embedding = self.token_embedder.compute_embedding(word)\n",
    "#             if word_embedding.shape[0] == self.token_embedder.d:\n",
    "#                 curr_vect.append(word_embedding)\n",
    "#         if len(curr_vect) == 0:\n",
    "#             curr_vect = np.zeros(self.token_embedder.d, dtype=float)  # Use the embedder's dimension\n",
    "#         else:\n",
    "#             curr_vect = np.mean(np.array(curr_vect), axis=0)\n",
    "\n",
    "#         # Convert to pytorch tensor\n",
    "#         curr_vect = torch.from_numpy(curr_vect).float()\n",
    "#         sentiment = self.sentiment.iloc[index]\n",
    "\n",
    "#         return curr_vect, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestReview(Dataset):\n",
    "#     def __init__(self, reviews, sentiment, token_embedder):\n",
    "#         \"\"\"\n",
    "#         Initializes the dataset with reviews, sentiment labels, and a token embedder.\n",
    "        \n",
    "#         :param reviews: A pandas Series or DataFrame column containing review texts.\n",
    "#         :param sentiment: A pandas Series or DataFrame column containing sentiment labels.\n",
    "#         :param token_embedder: An instance of the TokenEmbedder class for generating embeddings.\n",
    "#         \"\"\"\n",
    "#         self.reviews = reviews\n",
    "#         self.sentiment = sentiment\n",
    "#         self.token_embedder = token_embedder\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.reviews)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         curr_review = self.reviews.iloc[index]\n",
    "#         curr_review = curr_review.replace(',', '').replace('.', '').split()\n",
    "#         curr_vect = []\n",
    "\n",
    "#         for word in curr_review:\n",
    "#             # Use the TokenEmbedder's compute_embedding method to get the embedding for each word\n",
    "#             word_embedding = self.token_embedder.compute_embedding(word)\n",
    "#             if word_embedding.shape[0] == self.token_embedder.d:\n",
    "                \n",
    "#                 curr_vect.append(word_embedding)\n",
    "\n",
    "#         if len(curr_vect) == 0:\n",
    "#             curr_vect = np.zeros(self.token_embedder.d, dtype=float)  # Use the embedder's dimension\n",
    "#         else:\n",
    "#             curr_vect = np.mean(np.array(curr_vect), axis=0)\n",
    "\n",
    "#         # Convert to PyTorch tensor\n",
    "#         curr_vect = torch.from_numpy(curr_vect).float()\n",
    "#         sentiment = self.sentiment.iloc[index]\n",
    "\n",
    "#         return curr_vect, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_avg_custom_binary = TrainReview(X_train_raw_binary, Y_train_raw_binary, embedder)\n",
    "# test_data_avg_custom_binary = TestReview(X_test_raw_binary, Y_test_raw_binary, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how many samples per batch to load\n",
    "# batch_size = 100\n",
    "# # percentage of training set to use as validation\n",
    "# valid_size = 0.2\n",
    "\n",
    "# # obtain training indices that will be used for validation\n",
    "# num_train = len(train_data_avg_custom_binary)\n",
    "# indices = list(range(num_train))\n",
    "# np.random.shuffle(indices)\n",
    "# split = int(np.floor(valid_size * num_train))\n",
    "# train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# # define samplers for obtaining training and validation batches\n",
    "# train_sampler = SubsetRandomSampler(train_idx)\n",
    "# valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# # prepare data loaders\n",
    "# train_loader = torch.utils.data.DataLoader(train_data_avg_custom_binary, batch_size=batch_size, sampler=train_sampler)\n",
    "# valid_loader = torch.utils.data.DataLoader(train_data_avg_custom_binary, batch_size=batch_size, sampler=valid_sampler)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data_avg_custom_binary, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw_binary, X_test_raw_binary, Y_train_raw_binary, Y_test_raw_binary = train_test_split(downsized_df['review_body'], downsized_df['sentiment'], test_size=0.2, random_state=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainReviewCNN(Dataset):\n",
    "    def __init__(self, reviews, sentiment, token_embedder, max_length=50, vector_size=300):\n",
    "        self.reviews = reviews\n",
    "        self.sentiment = sentiment\n",
    "        self.token_embedder = token_embedder\n",
    "        self.max_length = max_length\n",
    "        self.vector_size = vector_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        curr_review = self.reviews.iloc[index]\n",
    "        curr_review = curr_review.replace(',', '').replace('.', '').split()\n",
    "        curr_vect = []\n",
    "        count = 0\n",
    "        for word in curr_review:\n",
    "            if count >= self.max_length:\n",
    "                break\n",
    "            word_embedding = self.token_embedder.compute_embedding(word)\n",
    "            # Ensure the embedding matches the expected vector size\n",
    "            if word_embedding.shape[0] != self.vector_size:\n",
    "                # If not, you might need to truncate or pad the embedding\n",
    "                if word_embedding.shape[0] > self.vector_size:\n",
    "                    word_embedding = word_embedding[:self.vector_size]  # Truncate\n",
    "                else:\n",
    "                    # Pad the embedding with zeros if it's shorter than expected\n",
    "                    padding = np.zeros(self.vector_size - word_embedding.shape[0], dtype=float)\n",
    "                    word_embedding = np.concatenate((word_embedding, padding))\n",
    "            curr_vect.append(word_embedding)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "        # if review is less than max_length words, append zeros\n",
    "        while count < self.max_length:\n",
    "            curr_vect.append(np.zeros(self.vector_size, dtype=float))\n",
    "            count += 1\n",
    "        if len(curr_vect) == 0:\n",
    "            curr_vect = np.zeros([self.max_length, self.vector_size], dtype=float)\n",
    "        else:\n",
    "            curr_vect = np.array(curr_vect)\n",
    "        curr_vect = np.transpose(curr_vect)\n",
    "        # Convert to pytorch tensor\n",
    "        curr_vect = torch.from_numpy(curr_vect).float()\n",
    "        sentiment = self.sentiment.iloc[index]\n",
    "    \n",
    "        return curr_vect, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestReviewCNN(Dataset):\n",
    "    def __init__(self, reviews, sentiment, token_embedder, max_length=50, vector_size=300):\n",
    "        self.reviews = reviews\n",
    "        self.sentiment = sentiment\n",
    "        self.token_embedder = token_embedder\n",
    "        self.max_length = max_length\n",
    "        self.vector_size = vector_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        curr_review = self.reviews.iloc[index]\n",
    "        curr_review = curr_review.replace(',', '').replace('.', '').split()\n",
    "        curr_vect = []\n",
    "        count = 0\n",
    "        for word in curr_review:\n",
    "            if count >= self.max_length:\n",
    "                break\n",
    "            word_embedding = self.token_embedder.compute_embedding(word)\n",
    "            # Ensure the embedding matches the expected vector size\n",
    "            if word_embedding.shape[0] != self.vector_size:\n",
    "                # If not, you might need to truncate or pad the embedding\n",
    "                if word_embedding.shape[0] > self.vector_size:\n",
    "                    word_embedding = word_embedding[:self.vector_size]  # Truncate\n",
    "                else:\n",
    "                    # Pad the embedding with zeros if it's shorter than expected\n",
    "                    padding = np.zeros(self.vector_size - word_embedding.shape[0], dtype=float)\n",
    "                    word_embedding = np.concatenate((word_embedding, padding))\n",
    "            curr_vect.append(word_embedding)\n",
    "            count += 1\n",
    "\n",
    "        # if review is less than max_length words, append zeros\n",
    "        while count < self.max_length:\n",
    "            curr_vect.append(np.zeros(self.vector_size, dtype=float))\n",
    "            count += 1\n",
    "        if len(curr_vect) == 0:\n",
    "            curr_vect = np.zeros([self.max_length, self.vector_size], dtype=float)\n",
    "        else:\n",
    "            curr_vect = np.array(curr_vect)\n",
    "        curr_vect = np.transpose(curr_vect)\n",
    "        # Convert to pytorch tensor\n",
    "        curr_vect = torch.from_numpy(curr_vect).float()\n",
    "        sentiment = self.sentiment.iloc[index]\n",
    "    \n",
    "        return curr_vect, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cnn_custom_binary = TrainReviewCNN(X_train_raw_binary, Y_train_raw_binary, embedder)\n",
    "test_data_cnn_custom_binary = TestReviewCNN(X_test_raw_binary, Y_test_raw_binary, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "valid_size = 0.2\n",
    "\n",
    "num_train = len(train_data_cnn_custom_binary)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data_cnn_custom_binary, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data_cnn_custom_binary, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_data_cnn_custom_binary, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TernaryCNN(nn.Module):\n",
    "    def __init__(self, output_channels1=50, output_channels2=10, max_length=50, vector_size=300, num_classes=3):\n",
    "        super(TernaryCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=vector_size, out_channels=output_channels1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=output_channels1, out_channels=output_channels2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "\n",
    "        self.fc1 = nn.Linear(120, num_classes)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TernaryCNN(\n",
      "  (conv1): Conv1d(300, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(50, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=120, out_features=3, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (dropout2): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "CustomTernaryCNN = TernaryCNN().to(device)\n",
    "print(CustomTernaryCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define two loss functions\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.MSELoss()\n",
    "\n",
    "# Define the weights for the loss functions\n",
    "weight1 = 0.7  # Weight for Cross Entropy Loss\n",
    "weight2 = 0.3  # Weight for Mean Squared Error Loss\n",
    "\n",
    "def combined_loss(output, target):\n",
    "    loss1 = criterion1(output, target)  # Use CrossEntropyLoss for classification\n",
    "    \n",
    "    # Reshape target tensor to match the output tensor\n",
    "    target_one_hot = F.one_hot(target, num_classes=output.size(1))\n",
    "    \n",
    "    # Convert target_one_hot to the appropriate format\n",
    "    target_one_hot = target_one_hot.float()\n",
    "    \n",
    "    loss2 = criterion2(output, target_one_hot)  # Use MSE loss with one-hot encoded target\n",
    "    return weight1 * loss1 + weight2 * loss2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(CustomTernaryCNN.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-10 00:28:37] Epoch: 1 \tTraining Loss: 0.570930 \tValidation Loss: 0.134788\n",
      "[2024-04-10 00:28:37] Validation loss decreased (inf --> 0.134788).  Saving model ...\n",
      "[2024-04-10 00:32:02] Epoch: 2 \tTraining Loss: 0.517385 \tValidation Loss: 0.128528\n",
      "[2024-04-10 00:32:02] Validation loss decreased (0.134788 --> 0.128528).  Saving model ...\n",
      "[2024-04-10 00:35:30] Epoch: 3 \tTraining Loss: 0.503388 \tValidation Loss: 0.124266\n",
      "[2024-04-10 00:35:30] Validation loss decreased (0.128528 --> 0.124266).  Saving model ...\n",
      "[2024-04-10 00:38:37] Epoch: 4 \tTraining Loss: 0.495885 \tValidation Loss: 0.122531\n",
      "[2024-04-10 00:38:37] Validation loss decreased (0.124266 --> 0.122531).  Saving model ...\n",
      "[2024-04-10 00:42:01] Epoch: 5 \tTraining Loss: 0.490666 \tValidation Loss: 0.121801\n",
      "[2024-04-10 00:42:01] Validation loss decreased (0.122531 --> 0.121801).  Saving model ...\n",
      "[2024-04-10 00:45:48] Epoch: 6 \tTraining Loss: 0.487153 \tValidation Loss: 0.123688\n",
      "[2024-04-10 00:49:32] Epoch: 7 \tTraining Loss: 0.483945 \tValidation Loss: 0.121192\n",
      "[2024-04-10 00:49:32] Validation loss decreased (0.121801 --> 0.121192).  Saving model ...\n",
      "[2024-04-10 00:52:36] Epoch: 8 \tTraining Loss: 0.481257 \tValidation Loss: 0.120640\n",
      "[2024-04-10 00:52:36] Validation loss decreased (0.121192 --> 0.120640).  Saving model ...\n",
      "[2024-04-10 00:55:45] Epoch: 9 \tTraining Loss: 0.478810 \tValidation Loss: 0.120427\n",
      "[2024-04-10 00:55:45] Validation loss decreased (0.120640 --> 0.120427).  Saving model ...\n",
      "[2024-04-10 00:58:52] Epoch: 10 \tTraining Loss: 0.476704 \tValidation Loss: 0.120571\n",
      "[2024-04-10 01:02:06] Epoch: 11 \tTraining Loss: 0.475231 \tValidation Loss: 0.120658\n",
      "[2024-04-10 01:05:17] Epoch: 12 \tTraining Loss: 0.473743 \tValidation Loss: 0.120761\n",
      "[2024-04-10 01:08:26] Epoch: 13 \tTraining Loss: 0.472040 \tValidation Loss: 0.120277\n",
      "[2024-04-10 01:08:26] Validation loss decreased (0.120427 --> 0.120277).  Saving model ...\n",
      "[2024-04-10 01:12:29] Epoch: 14 \tTraining Loss: 0.471058 \tValidation Loss: 0.119776\n",
      "[2024-04-10 01:12:29] Validation loss decreased (0.120277 --> 0.119776).  Saving model ...\n",
      "[2024-04-10 01:15:54] Epoch: 15 \tTraining Loss: 0.469704 \tValidation Loss: 0.119632\n",
      "[2024-04-10 01:15:54] Validation loss decreased (0.119776 --> 0.119632).  Saving model ...\n",
      "[2024-04-10 01:19:26] Epoch: 16 \tTraining Loss: 0.468364 \tValidation Loss: 0.119553\n",
      "[2024-04-10 01:19:26] Validation loss decreased (0.119632 --> 0.119553).  Saving model ...\n",
      "[2024-04-10 01:23:33] Epoch: 17 \tTraining Loss: 0.467919 \tValidation Loss: 0.120213\n",
      "[2024-04-10 01:27:15] Epoch: 18 \tTraining Loss: 0.466107 \tValidation Loss: 0.119725\n",
      "[2024-04-10 01:30:02] Epoch: 19 \tTraining Loss: 0.465742 \tValidation Loss: 0.120568\n",
      "[2024-04-10 01:32:51] Epoch: 20 \tTraining Loss: 0.465001 \tValidation Loss: 0.119846\n",
      "[2024-04-10 01:35:35] Epoch: 21 \tTraining Loss: 0.464483 \tValidation Loss: 0.119890\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     12\u001b[0m CustomTernaryCNN\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m, in \u001b[0;36mTrainReviewCNN.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     13\u001b[0m     curr_review \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreviews\u001b[38;5;241m.\u001b[39miloc[index]\n\u001b[0;32m---> 14\u001b[0m     curr_review \u001b[38;5;241m=\u001b[39m curr_review\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     15\u001b[0m     curr_vect \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "n_epochs = 30\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    CustomTernaryCNN.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.float().to(device)\n",
    "        target = target.long().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = CustomTernaryCNN(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = combined_loss(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "    CustomTernaryCNN.eval()\n",
    "    for data, target in valid_loader:\n",
    "        data = data.float().to(device)\n",
    "        output = CustomTernaryCNN(data)\n",
    "        target = target.long().to(device)\n",
    "        loss = combined_loss(output, target)\n",
    "        valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "\n",
    "    # Get current timestamp\n",
    "    current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    print('[{}] Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        current_time,\n",
    "        epoch + 1,\n",
    "        train_loss,\n",
    "        valid_loss\n",
    "    ))\n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('[{}] Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            current_time,\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "        torch.save(CustomTernaryCNN.state_dict(), 'CustomTernaryCNN.pt')\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    actual_list = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.float()\n",
    "        # inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.append(int(predicted[0]))\n",
    "        actual_list.append(int(targets[0]))\n",
    "    total = 0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] == actual_list[i]:\n",
    "            total += 1\n",
    "    accuracy = float(total) / len(prediction_list)\n",
    "    return accuracy\n",
    "\n",
    "CustomTernaryCNN.load_state_dict(torch.load('CustomTernaryCNN.pt'))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data_cnn_custom_binary, batch_size=1)\n",
    "print('Accuracy of CNN using average custom Rolling hash vectors (Ternary) :',str(predict(CustomTernaryCNN, test_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - \n",
    "2 - \n",
    "3 - \n",
    "4 - \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
